---
title: "FinBERT: Financial Sentiment Analysis"
date: 2026-01-25 20:40:00 +0900
categories: [PaperReview]
tags: [nlp, finance]
math: true
---
## Overview
> FinBERT = 금융 텍스트에 특화된 BERT 모델로, 문맥이해가 뛰어나서 기존 모델을이 놓치던 감정(특히 숨은 긍·부정)을 잘 캐치한다.

금융텍스트는 단어만 보면 중립처럼 보이는데, **문맥상 명확히 긍정or부정인 경우**가 많다. <br>
따라서 FinBERT는 문장 전체 맥락을 보고 판단하기에 도메인에 강점이 있다.

FinBERT의 성능을 평가하기 위해
- Loughran & McDonald 금융 사전
- 전통 ML(Naïve Bayes, SVM, Random Forest)
- 딥러닝 (CNN, LSTM) <br>
이들과 비교한 결과 FinBERT가 전부 이겼다. <br>
→ 기존 모델이 neutral로 잘못 분류한 문장을 FinBERT는 긍정/부정으로 정확히 분류

또한 FinBERT는 학습데이터가 적은 상황에서도 잘 버틴다.

## Introduction
금융·회계 분야에서도 NLP 연구가 많아졌는데, 대부분이 Bag-of-Words을 사용한다

**BoW의 특징**
- 문맥 무시
- 단어 순서 무시
- 문법 구조 무시
- only 단어가 몇 번 나왔는지만 확인

그러다가 최근들어, 딥러닝 기반 NLP가 등장했다 (ex.ELMo, GPT, BERT). <br>
이들은 Semantic(단어의 의미 관계), Syntatic(문법 구조, 어순)도 학습해서 문맥을 이해할 수 있다.
그러나 LLM은 비용이 많이 들고, 블랙박스라 모델의 판단 과정을 알 수가 없다.

해당 논문은 그래서 기존 방법, 전통 ML, 딥러닝 모두와 FinBERT를 성능을 비교하고자 한다.

일단 금융 텍스트는 일상 텍스트보다 전문 용어도 많고, 표현도 간접적이다. <br>
그리고 금융·회계 연구는 기존 LLM과제인 **질문 답변(QA), 번역, 개체명 인식(NER)**과 달리 **투자자에게 정보 전달력, 시장 반응**에 초점화 되어있다.

이러한 점들을 고려해서 해당 연구에서는 금융 텍스트에서 어떤 모델이 제일 좋은지, 얼마나 더 좋은지 실증분석할 것이다.

LLM을 학습시킬 때 보통 다음과 같은 과정을 거친다.
1. pretraining
2. fine-tuning <br>

따라서 해당 연구는 Google이 개발한 BERT를 pretrain해서 FinBERT를 만들었다.

## Deep learning NLP algorithms
_왜 수많은 딥러닝 모델 중 BERT를 사용했을까?_

1. 전통 ML보다 딥러닝 모델들이 성과가 뛰어남
2. 과거 임베딩은 static embedding으로 단어간 유사성을 학습해서, 특정 단어 주변에 어떤 단어들이 자주 등장하는지 파악했는데, <br> 단어 하나당 벡터1개만 표현해서 문맥이 달라도 항상 같은 표현인 문제가 있다. (다의어 구분X)
  - 예시 모델: word2vec, GloVe, fastText
3. contextualized embedding은 단어 벡터가 문맥에 따라 달라진다.
- 예시 모델: ElMo, GPT, BERT

딥러닝 모델들을 학습에 비용이 많이 들어서 기존에 학습된 모델에 **pretraining을 하거나 fin-tune**을 해서 비용은 줄이고 성능은 높이는 방법을 쓸 수 있다.

![FinBERT의 구조](\assets\img\posts\FinBERT_1.png)
_BERT를 기반으로 한 금융 분야 특화 모델 FinBERT의 파이프라인_

_FinBERT가 어떻게 학습되었는가?_<br>
**Pretraining**
- 데이터 > 총 49억 토큰
  - 10-K, 10-Q(기업공시): 각 60,490건, 142,622건
    - 대상: Russell 3000 기업
    - 기간: 1994–2019
    - 출처: SEC EDGAR
  - 애널리스트 리포트: 476,633건
    - 대상: S&P 500 기업
    - 기간: 2003–2012
    - 출처: Thomson Investext
  - conference calls: 136,578건
    - 대상: 7,740개 상장기업
    - 기간: 2004–2019
    - 출처: SeekingAlpha
- 방법 (BERT의 기본 구조랑 똑같)
  1.  Masked Language Model (MLM) > 단어 간 문맥 학습
  2. Next Sentence Prediction (NSP) > 문장 간 문맥 학습

**Fine-tuning**
- pretraining된 BERT위에 task specfic layer추가
- 적은 양의 라벨로 특정 과제 맞게 조정(ex. 문장분류, 개체 인식, QA)

## Comparing performance on sentiment classification
**sentiment classification 성능 비교 대상**
- FinBERT
- LM dictionary
- 전통 ML,딥러닝 모델
  - Naive Bayes
  - SVM
  - RF
  - CNN
  - LSTM

다음 모델들을 연구자가 감정을 직접 라벨링한 애널리스트 리포트에서 같은 학습 데이터, 테스트 데이터를 나눠서 성능을 비교 했다. 

**모델간 정확도 비교**
| Model | Accuracy (%) |
|--------------------|--------------|
| **FinBERT** | **88.2** |
| LM Dictionary | 62.1 |
| Naïve Bayes (NB) | 73.6 |
| Support Vector Machine (SVM) | 72.6 |
| Random Forest (RF)| 71.9 |
| Convolutional Neural Network (CNN) | 75.1 |
| Long Short-Term Memory (LSTM) | 76.3 |
| BERT | 85 |

**negative sentiment 파악**
| Model Category | Accuracy (%) |
|-----------------------|--------------|
| **FinBERT** | **89.7** |
| Non-BERT Algorithms | < 60.0 |

**FinBERT vs. Non-BERT Models**
| Model | Training Sample | Accuracy (%) |
|------------------|-----------------|--------------|
| **FinBERT** | 10% | **81.3** |
| LSTM | 100% | 76.3 |
| CNN | 100% | 75.1 |

**Sensitivity of BERT to Training Sample Size**
| Model | Training Sample | Accuracy (%) |
|-------|-----------------|--------------|
| BERT | 20% | 76.7 |
| BERT | 10% | 62.0 |

## Conclusion
** 학문적 기여**
- 금융 텍스트 분석 특화 LLM개발
** 실무적 함의**
- 투자 전문가의 리포트·공시·컨콜 분석 고도화
- 규제기관의 공시·ESG 모니터링 정확도 향상