---
title: "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing (2022)"
date: 2026-02-20 11:01:00 +0900
categories: [PaperReview]
tags: [NLP, BTM]
math: true
---
> **abstract**<br>
> 모든 task에 탁월한 범용모델 만들기<br> 
> **introduction**<br>
> 화자인식, 화자 분류 등 task가 다름에도 범용모델로 해결하는 것엔 어려움이 있다.<br> 
> **related works**<br>
> HuBERT, Wav2vec2.0<br>
> **method**<br>
>  masked speech prediction을 이용해서 화자인식 및 노이즈한 환경에서도 성능이 잘나오고, relative positionbias을 이용해서 인풋사이의 거리를 잘 인식한다.<br>  
> **experiment**<br>
> SUPERB에서 최고 성능 달성<br>
> **discussion**<br>
> 향후 과제로는 모델 크기를 더욱 확장하거나 압축하는 기술, 그리고 텍스트와 음성을 함께 사전 학습하는 통합 프레임워크 연구를 제시<br>

## Introduction

최근 음성 인식(ASR) 분야에서 자기지도 학습(Self-Supervised Learning, SSL)이 큰 성공을 거두었으나, 다른 음성 처리 task로 확장은 제한적이었다.

기존 사전 학습 모델인 HuBERT나 wav2vec2.0은 단일 화자 오디오로만 학습되어서 화자분할이다 음성 분리와 같은 다중 화자 task에서 성능이 떨어지며, 주로 오디오북 데이터에 편향되어서 실제 환경과의 불일치가 있다.

따라서 본 논문은 ASR뿐만 아니라 full-stack 음성 처리 태스크를 모두 해결할 수 있는 범용 사전 학습 모델인 WavLM을 제안한다.

## Related Work & Background

기존의 음성 SSL 모델(wav2vec 2.0, HuBERT 등)은 주로 콘텐츠 정보를 학습하여 음성 인식(ASR)에 초점을 맞췄다.

최근 SUPERB 벤치마크가 등장하여 다양한 태스크에서 SSL 모델을 평가하고 있으며, HuBERT가 좋은 성능을 보였다.<br>
HuBERT는 K-means 클러스터링을 통해 생성된 이산적인 타겟(pseudo-label)을 마스킹된 영역에서 예측하도록 모델을 학습시키는 모델로 WavLM은 이런 HuBERT의 구조를 기반으로 확장되었다.
> SUPERB(Speech processing Universal PERformance Benchmark): 음성 모델이 얼마나 '범용적인' 능력을 갖췄는지 평가하기 위해 만들어진 가장 대표적인 표준 평가 지표
> - Content (내용): 음성 인식(ASR), 음소 인식(PR) 등 "무슨 말인가?"를 평가
> - Speaker (화자): 화자 식별(SID), 화자 검증(SV) 등 "누가 말했는가?"를 평가
> -  Semantics (의미): 의도 파악(IC), 개체명 인식(SLU) 등 "어떤 뜻인가?"를 평가
> - Paralinguistics (부차 정보): 감정 인식(ER) 등 목소리에 담긴 분위기를 평가

## Method

WavLM은 범용적인 음성 표현 학습을 위해 구조, 학습 방법, 데이터셋을 크게 개선했다.

![WavLM 아키텍처](/assets/img/posts/WavLM.png)
WavLM은 **CNN인코더 + Transformer인코더**로 되어있다.

- CNN 인코더 구조
  - (CNN Layer + Normalization + GELU) * 7
- Transformer 인코더 구조
  - Relative Position Embedding + Gated Relative Position Bias

WavLM 아키텍처에서 가장 중요한 구조는 **Gated Relative Position Bias**와 **Masked Speech Denoising and Prediction**이다.

![Gated Relative Position Bias](/assets/img/posts/relative_bias.png)
![Bucket Relative Position Embedding](/assets/img/posts/relative_embedding.png)
- Gated Relative Position Bias: 기존의 합성곱 기반 위치 임베딩 대신, 입력된 음성 콘텐츠의 상태에 따라 상대적 위치 편향을 적응적으로 조절하는 게이트이다. 이를 통해 ASR 성능을 향상시켰다.
- Masked Speech Denoising and Prediction: 학습 배치의 일부 발화에 임의의 배경 노이즈나 다른 화자의 음성을 섞어 Overlapped speech를 인위적으로 사용했다. 모델은 섞인 음성에서 Main speaker를 식별하고 마스킹된 영역에서 원본 음성의 pseudo-label을 예측해야 한다.

> Pseudo-label: 정답이 없는 방대한 데이터에 모델이 예측한 값을 임시 정답으로 부여하여 이를 다시 학습에 활용하는 Self-supervised Learning이다.

**Data**<br>
기존 오디오북의 **데이터 한계**를 극복하고 다양한 음향 환겨와 화자 정보를 포함하기 위해 **94k 시간의 대규모 데이터셋**을 사용했다.
- LibriLight (60k 시간): 오디오북
- GigaSpeech (10k 시간): 팟캐스트, 유튜브 등
- VoxPopuli (24k 시간): 유럽 의회 회의 녹음 등

**Training**<br>
대용량 모델을 fp16(16-bit float precision)으로 학습할 때 발생하는 오버플로우(NaN loss) 문제를 해결하기 위해, Attention Score 계산 시 최댓값을 빼주는 스케일링 트릭을 적용하여 학습을 안정화했다.

## Results

![SUPERB결과](/assets/img/posts/SUPERB.png)
15개의 다양한 음성 하위 태스크(의미, 화자, 콘텐츠 파악 등)를 아우르는 SUPERB에서 최고 성능을 달성했다.<br>
3배나 크기가 작은 WavLM Base+ 모델이 기존의 HuBERT Large와 wav2vec 2.0 Large 모델을 뛰어넘는 전체 점수를 기록했다.

![WavLM의 레이어별 작업 기여도 히트맵](/assets/img/posts/SUPERB2.png)
15개의 다양한 음성 하위 태스크(의미, 화자, 콘텐츠 파악 등)
더하여 레이어별로 가중치를 분석해보니, **화자 관련 task(화자 인식, 화자 분할 등)에서는 모델의 하위 레이어**에서, **음성 인식 및 의미 관련 task는 상위 레이어**에서 주로 정보를 추출함을 확인했다.

**Speaker Verification(화자 인식)**

| Feature | Vox1-O | EER (%) Vox1-E | Vox1-H |
| :--- | :---: | :---: | :---: |
| ECAPA-TDNN [19] | 1.010 | 1.240 | 2.320 |
| ECAPA-TDNN (Ours) | 1.080 | 1.200 | 2.127 |
| **HuBERT Base** | 0.989 | 1.068 | 2.216 |
| **HuBERT Large** | 0.808 | 0.822 | 1.678 |
| **WavLM Base+** | 0.84 | 0.928 | 1.758 |
| **WavLM Large** | 0.617 | 0.662 | 1.318 |
| **HuBERT Large*** | 0.585 | 0.654 | 1.342 |
| **WavLM Large*** | **0.383** | **0.480** | **0.986** |

**Speaker Diariation(화자 분할)**

| Method | 2 | 3 | 4 | 5 | 6 | all |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| x-vector clustering [61] | 15.45 | 18.01 | 22.68 | 31.40 | 34.27 | 19.43 |
| SC-EEND [62] $^\ddagger$ | 9.57 | 14.00 | 21.14 | 31.07 | 37.06 | 15.75 |
| VBx [63] $^\dagger$ $^\ddagger$ | 9.44 | 13.89 | 16.05 | 13.87 | 24.73 | 13.28 |
| EEND-EDA [64] | 8.50 | 13.24 | 21.46 | 33.16 | 40.29 | 15.29 |
| EEND-EDA clustering [24] | 7.11 | 11.88 | 14.37 | 25.95 | 21.95 | 11.84 |
| EEND-vector clustering [65] | 7.96 | 11.93 | 16.38 | 21.21 | 23.10 | 12.49 |
| EEND-vector clustering (Ours) | 7.54 | 12.42 | 18.41 | 26.79 | 27.40 | 13.31 |
| --- | --- | --- | --- | --- | --- | --- |
| HuBERT Base & EEND-vector clustering | 7.93 | 12.07 | 15.21 | 19.59 | 23.32 | 12.63 |
| HuBERT Large & EEND-vector clustering | 7.39 | 11.97 | 15.76 | 19.82 | 22.10 | 12.40 |
| WavLM Base+ & EEND-vector clustering | 6.99 | 11.12 | 15.20 | 21.61 | 21.70 | 11.78 |
| **WavLM Large & EEND-vector clustering** | **6.46** | **10.69** | **11.84** | **12.89** | **20.70** | **10.35** |

나머지 **Speech Separation**랑 **Speech Recognition**에서도 좋은 성능을 보였다.(wav2vec 2.0 및 HuBERT와 동등하거나 더 우수한 ASR 성능을 보였다!)

## Conclusion

**Summary**<br>
WavLM은 대규모 데이터(94k 시간)와 '마스킹된 음성 예측 및 노이즈 제거' 기법을 결합하여, ASR뿐만 아니라 화자 관련 태스크 등 풀스택 음성 처리가 가능한 차세대 범용 백본(Backbone) 네트워크임을 증명했다.

**Contribution**<br>
- 마스킹된 음성 예측과 노이즈 제거(Denoising)를 결합한 새로운 프레임워크 도입
-  Gated Relative Position Bias를 Transformer 구조에 적용
- 오디오북 편향을 줄이기 위해 사전 학습 데이터를 94,000시간으로 확장